# (APPENDIX) APPENDICES {-}


# **Some Notes on Using `R`** 

You have probably heard many people say they should invest more time and effort to learn to use the `R` software environment for statistical computing... *and they were right*. However, what they probably meant to say is: "I tried it, but it's so damned complicated, I gave up"... *and they were right*. That is, they were right to note that this is not a point and click tool designed to accommodate any user. It was built for the niche market of scientists who use statistics, but in that segment it's actually the most useful tool I have encountered so far. 


## **New to `R`?** 

Now that your struggles with getting a grip on `R` are fully acknowledged in advance, let's try to avoid the 'giving up' from happening. Try to follow these steps to get started:   

1. **Get `R` and add some user comfort:** Install the latest [`R` software](http://www.r-project.org) *and* install a user interface like [RStudio](http://www.rstudio.com)... *It's all free!* An R interface will make some things easier, e.g., searching and installing packages from repositories. R Studio will also add functionality, like git/svn version control, project management and more, like the tools to create html pages like this one (`knitr` and `Rmarkdown`). Another source of user comfort are the `packages`. `R` comes with some basic packages installed, but you'll soon need to fit generalised linear mixture models, or visualise social networks using graph theory and that means you'll be searching for packages that allow you to do such things. A good place to start *package hunting* are the [CRAN task view](http://cran.r-project.org/web/views/) pages.

2. **Learn by running example `code`:** Copy the commands in the `code` blocks you find on this page, or any other tutorial or help files (e.g., Rob Kabacoff's [Quick R](http://www.statmethods.net)). Paste them into an `.R` script file in the script (or, source) editor. In R Studio You can run code by pressing `cmd` + `enter` when the cursor is on a single single line, or you can run multiple lines at once by selecting them first. If you get stuck remember that there are expert `R` users who probably have answered your question already when it was posted on a forum. Search for example through the Stack overflow site for [questions tagged with `R`](http://stackoverflow.com/questions/tagged/r))

3. **Examine what happens... when you tell `R` to make something happen:** `R` stores variables (anything from numeric data to functions) in an `Environment`. There are in fact many different environments, but we'll focus on the main workspace for the current `R` session. If you run the command `x <- 1+1`, a variable `x` will appear in the `Environment` with the value `2` assigned to it. Examining what happens in the `Environment` is not the same as examining the output of a statistical analysis. Output in `R` will appear in the `Console` window. Note that in a basic set-up each new `R` session starts with an empty `Environment`. If you need data in another session, you can save the entire `Environment`, or just some selected variables, to a file (`.RData`).

4. **Learn about the properties of `R` objects:** Think of objects as containers designed for specific content. One way to characterize the different objects in `R` is by how picky they are about the content you can assign it. There are objects that hold `character` and `numeric` type data, a `matrix` for numeric data organised in rows and columns, a `data.frame` is a matrix that allows different data types in columns, and least picky of all is the `list` object. It can carry any other object, you can have a `list` of which item 1 is an entire `data.frame` and item 2 is just a `character` vector of the letter `R`. The most difficult thing to master is how to efficiently work with these objects, how to assign values and query contents.

5. **Avoid repeating yourself:** The `R` language has some amazing properties that allow execution of many repetitive algorithmic operations using just a few lines of code at speeds up to warp 10. Naturally, you'll need to be at least half Vulcan to master these features properly and I catch myself copying code when I shouldn't on a daily basis. The first thing you will struggle with are the `apply` functions. These functions pass the contents of a `list` object to a function. Suppose we need to calculate the means of column variables in 40 different SPSS `.sav` files stored in the folder `DAT`. With the `foreign` package loaded we can execute the following commands:   
`data <- lapply(dir("/DAT/",pattern=".sav$"),read.spss)`        
`out  <- sapply(data,colMeans)`       
The first command applies read.spss to all files with a `.sav` extension found in the folder `/DAT`. It creates a data frame for each file which are all stored as elements of the list `data`. The second line applies the function `colMeans` to each element of `data` and puts the combined results in a matrix with dataset ID as columns (1-40), dataset variables as rows and the calculated column means as cells. This is just the beginning of the `R` magic, wait 'till you learn how to write functions that can create functions.


## **Getting started with `R` tutorials**  {#tutorials}

* Tutorials on using **functions**:
     + [Quick-R](https://www.statmethods.net/management/userfunctions.html)
     + [Software Carpentry](https://swcarpentry.github.io/r-novice-inflammation/02-func-R/)
     + [Nicer Code](https://nicercode.github.io/guides/functions/)
     + [Advanced R](http://adv-r.had.co.nz/Functions.html)

* Tutorials on using **conditionals** and **for loops**:
     + [Quick-R](https://www.statmethods.net/management/controlstructures.html)
     + [Software Carpentry](https://swcarpentry.github.io/r-novice-inflammation/15-supp-loops-in-depth/)
     + [R-Bloggers](https://www.r-bloggers.com/how-to-write-the-first-for-loop-in-r/)

* Tutorials on the **-ply family** of functions:
      + [R-bloggers](https://www.r-bloggers.com/using-apply-sapply-lapply-in-r/)
      + [Nicer Code](https://nicercode.github.io/guides/repeating-things/)
      + [R for Dummies](http://www.dummies.com/programming/r/how-to-use-the-apply-family-of-functions-in-r/)

* **Plotting**, **plotting** and more **plotting**:
      + [A Compendium of Clean Graphs in R](http://shinyapps.org/apps/RGraphCompendium/index.php)
      + [ggplot2 reference](http://ggplot2.tidyverse.org/reference/)
      + [ggplot2 extensions](http://www.ggplot2-exts.org/gallery/)
      + [patchwork, the ultimate ggplot2 extension](https://github.com/thomasp85/patchwork)
      + [The R-graph gallery](https://www.r-graph-gallery.com)
      + [Quick-R](https://www.statmethods.net/graphs/index.html)
      + [Nicer Code](https://nicercode.github.io/guides/plotting/)
  
* Tutorial on **Effect Size Confidence Intervals** and more:
     + In this tutorial on [estimating Effect Size Confidence Intervals (ESCI)](http://fredhasselman.com/post/2015-05-05-OSC-R-ESCI-Tutorial/) there are a lot of examples on how to use `R`. 
     + It was written as an addendum for [a post](http://centerforopenscience.github.io/osc/2014/03/06/confidence%20intervals/) on the **Open Science Collaboration Blog**, which contains many interesting entries on diverse subjects (like [behavioural priming](http://centerforopenscience.github.io/osc/2014/03/26/behavioral-priming/), [theoretical amnesia](http://centerforopenscience.github.io/osc/2013/11/20/theoretical-amnesia/) and [anonymous peer review](http://centerforopenscience.github.io/osc/2014/05/15/anonymous-peer-review/)) 


## **Not new to `R`?** 

If you have been using `R` for a while, but do not consider yourself a master yet, I recommend learning to use the [**tidyverse**](http://tidyverse.org) packages and the accompanying web-book [**R for data scientists**](http://r4ds.had.co.nz).

* Welcome to the **tidyverse**:
      + [Install the tidyverse](https://www.tidyverse.org)
      + [Learn how to use the tidyverse](http://r4ds.had.co.nz)
      + [Learn how to use the tidyverse to do statistics](http://moderndive.com)
      + [Learn how to use the tidyverse to create networks](https://www.rdocumentation.org/packages/tidygraph/versions/0.1.0)
      + [How to make R purrr](http://purrr.tidyverse.org)


## **Time series analyses in R** 

In this book you can find some tips on plotting time series (see section [**Working with time series in R**](#plotTS)) and we will be using package [`casnet`](https://fredhasselman.github.io/casnet/) as our main tool for analyses. However, if you really want a deep dive into everything related to time series in R be sure to check the CRAN task view on time series: https://cran.r-project.org/web/views/TimeSeries.html     

### **Installing casnet** {-}

To install `casnet` you need to have package `devtools` or `remotes` installed and call the following code from the commands line:

```{r echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
library(devtools)
devtools::install_github("FredHasselman/casnet", dependencies = TRUE)

# or equivalently
library(devtools)
remotes::install_github("FredHasselman/casnet", dependencies = TRUE)
```

If all goes well this should install the package and all the packages it depends on.
If the vignette build fails, don't worry, you can access them through the [casnet website](https://fredhasselman.com/casnet/) under *Articles*.



```{r setup, include = FALSE}
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height = 6,
	fig.width = 7,
	message = FALSE,
	warning = FALSE,
	collapse = FALSE,
	comment = ">",
	width = 500,
	dpi = 72
)
library(invctr)
library(casnet)
library(plyr)
library(tidyverse)

# https://addi.ehu.es/bitstream/handle/10810/19052/TFM-MALUnaiGarciarena.pdf?sequence=1&isAllowed=y 
# https://gking.harvard.edu/amelia 
# https://cran.r-project.org/web/packages/HotDeckImputation/HotDeckImputation.pdf 
```

# **Working with time series in R** {#plotTS}

There are many ways to handle time series in `R`, this appendix provides some examples and suggest some best practices, based on the function `ts()`, which creates a time series object.

A time series object is expected to have a time-dimension on the x-axis. This is very convenient, because `R` will generate the time axis for you by looking at the *t*ime *s*eries *p*roperties attribute of the object. Even though we are not working with measurement outcomes, consider a value at a time-index in a time series object a **sample**:

* `Start` -  The value of time at the first sample in the series (e.g., $0$, or $1905$)
* `End` - The value of time at the last sample in the series (e.g., $100$, or $2005$)
* `Frequency` - The amount of time that passed between two samples, or, the sample rate (e.g., $0.5$, or $10$)

Examples of using the time series object.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(2718282)
# Get a timeseries of 100 random numbers 
Y <- ts(rnorm(100))
# plot.ts
plot(Y)
# Get sample rate info
tsp(Y)
# Extract the time vector
time(Y)
```

For now, these values are in principle all arbitrary units (`a.u.`). These settings only make sense if they represent the parameters of an actual measurement procedure.

It is easy to adjust the time vector, by assigning new values using `tsp()` (values have to be possible given the time series length). For example, suppose the sampling frequency was $0.1$ instead of $1$ and the Start time was $10$ and End time was $1000$.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Assign new values
(tsp(Y) <- c(10, 1000, .1))
# Time axis is automatically adjusted 
time(Y)
```

## Plotting a `ts` object as a time series 

Depending on which packages you use, there will be different settings applied to time series objects created by `ts()`. Below are some examples of differences between plotting routines.

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, tidy=FALSE}
require(lattice)       # Needed for plotting
require(latticeExtra)  # Needed for plotting
require(casnet)        # Need for ts_center()

# stats::plot.ts
plot(Y, lwd = 2, main = "stats::plot.ts")
# lattice::xyplot.ts
xyplot(Y, lwd = 2, main = "lattice::xyplot.ts")
```

## Plotting multiple time series in one figure

Plot multiple time series in frames with `plot.ts()` in `package::stats`.
This function takes a matrix as input, here we use `cbind( ... )`.
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, tidy=FALSE}
# stats::plot.ts  
plot(cbind(Y,
           cumsum(Y), 
           cumsum(ts_center(Y))
           ), 
     yax.flip = TRUE, col = "blue", frame.plot = TRUE, 
     main = expression(paste("Random Numbers: ",N(0,sigma))), xlab = "time (a.u.)")
```

Plot multiple time series in one graph with `ts.plot()` in `package::graphics`.
This function can handle multiple `ts` objects as arguments.
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, tidy=FALSE}
# graphics::ts.plot
ts.plot(Y,
        cumsum(Y), 
        cumsum(ts_center(Y)),
        gpars = list(xlab = "time (a.u.)",
                     ylab = expression(Y(t)),
                     main = expression(paste("Random Numbers: ",N(0,sigma))),
                     lwd = rep(2,3),
                     lty = c(1:3),
                     col = c("darkred","darkblue","darkgreen")
                     )
        )
legend(0, 18, c("Y","cumsum(Y)", "cumsum(ts_center(Y))"), lwd = rep(2,3), lty = c(1:3), col = c("darkred","darkblue","darkgreen"), merge = TRUE, cex=.9)
```

Use `xyplot()` in `package::lattice` to create a plot with panels. The easiest way to do this is to create a dataset in so-called "long" format. This means the variable to plot is in 1 column and other variables indicate different levels, or conditions under which the variable was observed or simulated.

Function `ldply()` is used to generate $Y$ for three different settings of $r$. The values of $r$ are passed as a **l**ist and after a function is applied the result is returned as a **d**ataframe. 
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, tidy=FALSE}
require(plyr)          # Needed for function ldply()

# Create a long format dataframe for various values for `r`
data <- cbind.data.frame(Y     = c(as.numeric(Y), cumsum(Y), cumsum(ts_center(Y))),
                         time  = c(time(Y), time(Y), time(Y)),
                         label = factor(c(rep("Y",length(Y)),  rep("cumsum(Y)",length(Y)), rep("cumsum(ts_center(Y))",length(Y))))
                         )
# Plot using the formula interface
xyplot(Y ~ time | label, data = data, type = "l", main = expression(paste("Random Numbers: ",N(0,sigma))))
```

Or, if you have very may time series, you can use the function `PLOT()` in `casnet`.

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, tidy=FALSE}
# Create a data frame with time series
# Generate some coloured noise
N <- 512
noises <- seq(-2,2,by=.2)
y <- data.frame(matrix(rep(NA,length(noises)*N), ncol=length(noises)))

for(c in seq_along(noises)){y[,c] <- noise_powerlaw(N=N, alpha = noises[c])}
colnames(y) <- paste0(noises)

plotTS_multi(y)

```

Note that the y-axis is rescaled for each series and does not reflect magnitude differences between the series.



## The return plot

To create a return plot the values of $Y$ have to be shifted by a certain lag. The functions `lead()` and `lag()` in `package::dplyr` are excellent for this purpose (note that `dplyr::lag()` behaves different from `stats::lag()`).
```{r, echo=TRUE, message=FALSE, warning=FALSE, cache=FALSE, collapse=TRUE, tidy=FALSE}
# Function lag() and lead()
library(dplyr)
library(casnet)

# Get exponential growth
YY <- growth_ac(N=1000,r=1.5,type = "driving")
Y1 <- as.numeric(YY/max(YY))
# Get logistic growth in the chaotic regime
Y2 <- as.numeric(growth_ac(N=1000,r=4,type = "logistic"))
# Use the `lag` function from package `dplyr`
op <- par(mfrow = c(1,2), pty = "s")
plot.ts(dplyr::lag(Y1), Y1, xy.labels = FALSE, pch = 16, xlim = c(0,1), ylim = c(0,1), xlab = "Y(t)", ylab = "Y(t+1)",
     main = "exponential / max")
plot.ts(dplyr::lag(Y2), Y2, xy.labels = FALSE, pch = 16, xlim = c(0,1), ylim = c(0,1), xlab = "Y(t)", ylab = "Y(t+1)",
     main = "logistic / max")
par(op)
```

Use `l_ply()` from `package::plyr` to create return plots with different lags. The **l_** before **ply** means the function will take a **l**ist as input to a function, but it will not expect any data to be returned, for example in the case of a function that is used to plot something.

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=FALSE, figure.height=20, figure.width=20, tidy=FALSE}
# Explore different lags
op <- par(mfrow = c(1,2), pty = "s")
plyr::l_ply(1:4, function(l) plot.ts(dplyr::lag(Y2, n = l), Y2, xy.labels = FALSE, pch = 16, xlim = c(0,1), ylim = c(0,1), xlab = "Y(t)", ylab = paste0("Y(t+",l,")"), cex = .8))
par(op)
```

## Using `ggplot2`

Becoming proficient at `ggplot2` can take some time, but it does pay off. One of the problems with plotting time series data is that `ggplot2` wants tidy data in *long* format. Tidy data is:

> Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is
messy or tidy depending on how rows, columns and tables are matched up with observations,
variables and types. In tidy data:
1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.
>
> ---@wickham2014a

So if we have a set of time series as in the previous examples, we need to change it to long format.

```{r message=FALSE, warning=FALSE}
library(tidyverse)

# A wide data frame
df.wide <- data.frame(rnormY        = Y,
                      cumsumY       = cumsum(Y), 
                      centercumsumY = cumsum(ts_center(Y)),
                      time          = seq_along(Y)
                      )

glimpse(df.wide)

# Create a long dataframe using gather()
df.long <- df.wide %>% 
  gather(key=TimeSeries,value=Y,-"time")

glimpse(df.long)

# 1 plot
ggplot(df.long, aes(x=time, y=Y, colour=TimeSeries)) +
  geom_line() +
  theme_bw()

# using facets
ggplot(df.long, aes(x=time,y=Y)) +
  geom_line() + 
  facet_wrap(~TimeSeries) +
  theme_bw()

# using facets
ggplot(df.long, aes(x=time,y=Y)) +
  geom_line() + 
  facet_grid(TimeSeries~.) +
  theme_bw()

```

To create a return plot you can use `geom_path()` instead of `geom_line()` and make the area square using `coord_equal()`.

```{r message=FALSE, warning=FALSE}
# Add a lagged variable
df.long <- df.long %>%
  group_by(TimeSeries) %>%
  mutate(Ylag = dplyr::lag(Y))

# Use geom-path()
ggplot(df.long, aes(x=Y,y=Ylag,group=TimeSeries)) +
  geom_path() + 
  facet_grid(.~TimeSeries) +
  theme_bw() +
  labs(title = "Equal coordinates", x="Yt",y="Yt+1") +
  coord_equal()

# You could also have free axes
ggplot(df.long, aes(x=Y,y=Ylag,group=TimeSeries)) +
  geom_path() + 
  facet_grid(.~TimeSeries, scales = 'free') +
  labs(title="Free axes", x="Yt",y="Yt+1") +
  theme_bw() 

# Or free axes and a free space
ggplot(df.long, aes(x=Y,y=Ylag,group=TimeSeries)) +
  geom_path() + 
  facet_grid(.~TimeSeries, scales = 'free', space = 'free') +
  labs(title="Free axes and free space", x="Yt",y="Yt+1") +
  theme_bw() 
```


# **Dealing with Missing Data** {#Missing}


In an article called ["Much ado about nothing"?](http://www.math.smith.edu/~nhorton/muchado.pdf) several different approaches, methods and best practices for dealing with missing data are discussed. The methods are diverse, both in number and in their effect on the results of analyses. Therefore, the first rule of dealing with missing data is: **Always report analysis results for the imputed data as well as the data with missing values removed!**

The [CRAN taskview on missing data](https://cran.r-project.org/web/views/MissingData.html) is a good starting point for finding what you may need. In this vignette we will specifically discuss package [imputeTS](https://cran.r-project.org/web/packages/imputeTS/index.html) for *Time Series Missing Value Imputation* and [mice](https://cran.r-project.org/web/packages/mice/index.html) *Multivariate Imputation by Chained Equations*.


### Data with missing values {-}

We'll create some variables from which we artifically remove datapoints. This allows us to evaluate how well the imputation methods perform in recovering the true values. 

```{r}
set.seed(54321)
# Random normally distributed numbers
zscore <- rnorm(n = 122)
df_vars <- data.frame(zscore = zscore)
# Random discrete uniform numbers
df_vars$unif_discrete <- unif_discrete  <- round(runif(NROW(df_vars),min = 0,max = 6))
df_vars$unif_discrete[c(5,10:15,74:78,102,111,120)] <- NA
# Unordered catagorical 
df_vars$cat_unordered <- cat_unordered  <- factor(round(runif(NROW(df_vars),min = 1,max = 7)))
df_vars$cat_unordered[c(5,10:15,74:78,102,111,120)] <- NA
# Ordered categroical
df_vars$cat_ordered <- cat_ordered <- ordered(round(runif(NROW(df_vars),min = 1,max = 20)))
```

We'll also load the data analysed by Bastiaansen et al. (2019) and select some variables which have missing values.

```{r}
# # Load data from OSF https://osf.io/tcnpd/
# require(osfr)
# manyAnalystsESM <- rio::import(osfr::osf_download(osfr::osf_retrieve_file("tcnpd") , overwrite = TRUE)$local_path)

# Or use the internal data
data(manyAnalystsESM)

# We want to use these variables
# Note: the infix function '%ci%' is from package 'invctr'
vars    <- c("angry"%ci%manyAnalystsESM,"ruminate"%ci%manyAnalystsESM,"hours"%ci%manyAnalystsESM)

df_vars <-  cbind(df_vars,manyAnalystsESM[,vars])

# Give zscore and ordered categorical gthe same NAs as variable 'angry'
df_vars$zscore[is.na(df_vars$angry)] <- NA
df_vars$cat_ordered[is.na(df_vars$angry)] <- NA
```



Function `imputeTS::statsNA()` can produce some helpful statistics on the `NA`s that might be present in your data.

```{r}
require(imputeTS)

# The variable 'angry'
imputeTS::statsNA(df_vars$angry)

# Uniform discrete numbers
imputeTS::statsNA(df_vars$unif_discrete)

```


## Univariate imputation 

In addition to useful summary and visualisation tools, package `imputeTS` contains a number of imputation methods that are commonly used. If you have installed the package, run `vignette("imputeTS-Time-Series-Missing-Value-Imputation-in-R", package = "imputeTS")` from the console ans learn about all the options.


### Linear interpolation {-}

One of the most straightforward inputation methods is linear interpolation. This is a relatively sensible method if there is just one time point missing. However, when several values are missing in a row, the linear interpolation might be unrealistic. Other methods that will give less plausible results for imputation of multiple missing values in a row are *last observation carried forward* and *next observation carried backward*, also available in `imputeTS` as `na_locf(type = "locf")`, and `na_locf(type = "nocb")` respectively.

We'll generate a data set with linear interpolation (also available are `spline` and `stine` interpolation), to compare to the more advanced multiple imputation methods discussed below.

```{r}
out.linear <- t(laply(1:NCOL(df_vars), function(c){
  y  <- as.numeric(as.numeric_discrete(x = df_vars[,c], keepNA = TRUE))
  idNA <- is.na(y)
  yy <- cbind(imputeTS::na_interpolation(y,option = "linear"))
  if(all(is.wholenumber(y[!idNA]))){
    return(round(yy))
  } else {
      return(yy)
    }
  }))
colnames(out.linear) <- colnames(df_vars)
```

Note that we need to round the imputed values to get discrete values if the original variable was discrete.

### Kalman filter {-}

Imputation by using the Kalman filter is a powerful method for imputing data. However, when dealing with discrete data, one has to take some additional steps in order to get meaningful results. 

For example, with uniform discrete numbers and/or scales that are bounded (eg. visual analog scale from `0-100`), the Kalman method will not correctly impute the data and might go outsdide the bounds of the scale.

```{r}
# Use casnet::as.numeric_discrete() to turn a factor or charcter vector into a named numeric vector.
ca <- as.numeric_discrete(df_vars$cat_ordered, keepNA = TRUE)
imputations <- imputeTS::na_kalman(ca, model = "auto.arima")
imputeTS::ggplot_na_imputations(x_with_na = ca, x_with_truth = as.numeric_discrete(cat_ordered), x_with_imputations = imputations)
```

There is a way to adjust the imputation procedure by transforming the data (thanks to Steffen Moritz, author of `imputeTS`, for suggesting this method). The ordered categorical series was created with bounds `1` and `20`.

```{r}
# Bounds 
lo <- 1
hi <- 20
# Transform data, take care of dividsion by 0
ca_t <- log(((ca-lo)+.Machine$double.eps)/((hi-ca)+.Machine$double.eps))
imputations <- imputeTS::na_kalman(ca_t, model = "auto.arima")
# Plot the result
# Back-transform the imputed forecasts 
imputationsBack <- (hi-lo)*exp(imputations)/(1+exp(imputations)) + lo

imputeTS::ggplot_na_imputations(x_with_na = ca, x_with_truth = as.numeric_discrete(cat_ordered), x_with_imputations = imputationsBack)
```


## Multiple imputation

Package [mice](https://cran.r-project.org/web/packages/mice/index.html) implements a method called: *Multivariate Imputation by Chained Equations*. The main function `mice()` will try to select an appropriate method based on the type of variable (discrete, continuous, etc.). In general, the advantage of using `mice()` with discrete data is that it has a number of methods that will actually return discrete values.

Check the manual page for mice (e.g. type `?mice` in the console), to see the 25 methods that are available. On that manual page you can also find links to a number of vignettes that provide a very thorough explanation of all the functions the package has to offer. 

In this vignette, we will focus on a simple demonstration of just a few of the methods in `mice()`.


### Auto-select method {-}

We can just provide the `mice()` function our data set and it will take care of analysing the variables and selecting an appropriate imputation method. 

```{r message=FALSE, warning=FALSE}
require(mice)
# auto choice by mice algorithm
imp.mice <- mice::mice(df_vars)
```

The algorithm chooses methods `pmm`, `polyreg` and `polr`:

```{r}
imp.mice$method
```

By default `mice()` will generate `5` iterations of each time series, that is, argument `maxit = 5`. If you inspect the `imp.mice` object you can see it is a list with several fields, the field `imp` is another list with fields named after the columns in our data set. Each field contains `5` iterations for the variable.

```{r}
lengths(imp.mice$imp)
```

To generate replacements for the missing values from those `5` iterations we need to call the function `complete()`. 

```{r}
out.auto <- mice::complete(imp.mice)
```

Check the `complete()` manual entry for some other interesting options.


### Classification & regression trees {-}

We also choose an imputation method for all variables, one based on classification and regression trees (`cart`), it will give the same results as the method based on random forest imputation (`rf`).

```{r}
# RF and CART return (identical) discrete numbers
imp.cart  <- mice(df_vars, meth = 'cart', printFlag = FALSE)
out.cart  <- complete(imp.cart)

# imp.rf  <- mice(df_vars, meth = 'rf')
# out.rf  <- complete(imp.cart)
```


## Compare different imputation methods

We can check "truth" values for the variables we created, which obviously cannot be done for the empirical data.

### Visual inspection {-}

Function `imputeTS::ggplot_na_imputations()` is an excellent way to visualise the imputation result.

```{r, fig.height=10, fig.width=8, collapse=TRUE}
truth <- list(zscore, unif_discrete, cat_unordered, cat_ordered, df_vars$angry, df_vars$ruminate, df_vars$hours)
g1 <- g2 <- g3 <- list()

for(c in 1:NCOL(df_vars)){
  
withNA  <- as.numeric_discrete(df_vars[,c], keepNA = TRUE)
Truth   <- as.numeric_discrete(truth[[c]], keepNA = TRUE)

g1[[c]] <- ggplot_na_imputations(x_with_na = withNA, 
                 x_with_imputations = out.linear[,c], 
                   x_with_truth = Truth,
                   title = "linear interpolation",
                   ylab = colnames(df_vars)[c], legend = FALSE)
  
g2[[c]] <- ggplot_na_imputations(x_with_na = withNA, 
                 x_with_imputations = as.numeric_discrete(out.auto[,c]), 
                   x_with_truth = Truth,
                   title = paste("auto:",imp.mice$method)[c],
                   ylab = colnames(df_vars)[c], legend = FALSE)
  
g3[[c]] <- ggplot_na_imputations(x_with_na = withNA, 
                   x_with_imputations = as.numeric_discrete(out.cart[,c]), 
                     x_with_truth =Truth,
                     title="regression trees",
                     ylab = colnames(df_vars)[c])

print(colnames(df_vars)[c])
print(cowplot::plot_grid(g1[[c]],g2[[c]],g3[[c]], ncol = 1))

}

```


### Effect on analysis results {-}

Finally, we compare the effect of different methods on the results of analyses.

```{r echo=TRUE, collapse=TRUE, paged.print=FALSE}
df <- list()

for(c in 1:NCOL(df_vars)){
  
withNA  <- as.numeric_discrete(df_vars[,c], keepNA = FALSE)
Truth   <- as.numeric_discrete(truth[[c]], keepNA = FALSE)
LINEAR  <- as.numeric_discrete(unname(out.linear[,c]))
AUTO    <- as.numeric_discrete(out.auto[,c])
CART    <- as.numeric_discrete(out.cart[,c])


df[[c]] <- data.frame(
                 NAremoved = c(mean(withNA, na.rm = TRUE), sd(withNA,na.rm = TRUE)),
                 Truth     = c(mean(Truth,  na.rm = TRUE), sd(Truth,na.rm = TRUE)),
                 LINEAR    = c(mean(LINEAR, na.rm = TRUE), sd(LINEAR,na.rm = TRUE)),
                 AUTO      = c(mean(AUTO,   na.rm = TRUE), sd(AUTO,na.rm = TRUE)),
                 CART      = c(mean(CART,   na.rm = TRUE), sd(CART,na.rm = TRUE)))
rownames(df[[c]]) <- c("Mean","SD")
}

```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE, collapse=TRUE}

kableExtra::kable_minimal(kable(df[[1]], row.names = TRUE, caption = colnames(df_vars)[1],digits = 3))

kableExtra::kable_minimal(kable(df[[2]], row.names = TRUE, caption = colnames(df_vars)[2],digits = 3)) 

kableExtra::kable_minimal(kable(df[[3]], row.names = TRUE, caption = colnames(df_vars)[3],digits = 3))   

kableExtra::kable_minimal(kable(df[[4]], row.names = TRUE, caption = colnames(df_vars)[4],digits = 3))    

kableExtra::kable_minimal(kable(df[[5]], row.names = TRUE, caption = colnames(df_vars)[5],digits = 3))    

kableExtra::kable_minimal(kable(df[[6]], row.names = TRUE, caption = colnames(df_vars)[6],digits = 3))    

kableExtra::kable_minimal(kable(df[[7]], row.names = TRUE, caption = colnames(df_vars)[7],digits = 3))   
```


## References {-}

Bastiaansen, J. A., Kunkels, Y. K., Blaauw, F., Boker, S. M., Ceulemans, E., Chen, M., … Bringmann, L. F. (2019, March 21). Time to get personal? The impact of researchers’ choices on the selection of treatment targets using the experience sampling methodology. https://doi.org/10.31234/osf.io/c8vp7



# **List of terms** {#Terms}

### **Adaptive Behaviour** {- #Adap1}

+ System behaviour that appears to be (partially) coordinated by previously 'experienced events'

### **Analytic solution** {- #Anal2}

+ The solution to a difference or differential equation allows one to find any state of the system without the need to iterate the model starting from some initial condition. There are very few (systems of) equations for which analytical solutions exist

### **Attractor** {- #Attr3}

+ The status that a dynamic system eventually "settles down to". An attractor is a set of values in the phase space to which a system migrates over time, or iterations. Attractors can have as many dimensions as the number of variables that influence its system

### **Basin of attraction** {- #Basi4}

+ A region in phase space associated with a given attractor. The basin of attraction of an attractor is the set of all (initial) points that eventually end up in that attractor

### **Behaviour (of a dynamic system)** {- #Beha5}

+ The  temporal evolution of states of a system according to one or more rules (also known as state propagation rules, or, iterative processes). Models of the behaviour of dynamic systems use difference or differential equations to describe the iterative processes hypothesized to underlie the temporal evolution

### **Bifurcation** {- #Bifu6}

+ A clearly observable qualitative change in the behavioural mode (attractor state) of a dynamic system associated with continuous change in one or more control parameters  (also known as Phase-, State-, or Order- Transition). The value of a control parameter at which a bifurcation occurs is often non-specific, or trivial

### **Bifurcation diagram** {- #Bifu7}

+ Visual summary of the succession of period-doubling bifurcations produced by gradual changes in the control parameter(s)

### **Catastrophe flags** {- #Cata8}

+ Markers indicative for a physical system that is described by a catastrophe. There are 5 'classical' flags and 3 'diagnostic' flags. Classical: bimodality, sudden jumps, inaccessibility, sensitivity & hysteresis. Diagnostic: divergence from linear response, critical slowing down and critical  fluctuations. Diagnostic flags can be used as early-warning signals. 

### **Catastrophe theory** {- #Cata9}

+ Mathematical research program describing how gradual change in some parameters can lead to disproportionately large changes in another parameter, called catastrophes (similar to bifurcations, Phase-, State-, or Order-transitions). 'This kind of behaviour has been summarized succinctly in the phrase "the straw that broke the camel's back".' (Gilmore, 1992). 

### **Complex Network** {- #Comp10}

+ A network with of many nodes and likely many substructures depending on that nature and distribution of connections between nodes

### **Complex system** {- #Comp11}

+ Spatially and/or temporally extended nonlinear systems characterized by emergent properties and self-organised behavioural modes at a global, or, macro-level (the system as a whole), that is often different from the characteristic behaviour at a local, or, micro-level (behaviour of the individual parts that constitute the whole)

### **Complexity science** {- #Comp12}

+ Complexity science studies how systems that consist of many components can generate relatively simple and stable (non-random) behaviour. Important behavioural phenomena studied in Complexity Science are synchronisation, adaptation and coordination of behaviour across many different temporal and spatial scales, emergent properties and collective behaviour, holism and self-organisation

### **Component dominant dynamics** {- #Comp13}

+ A causal ontology in which observed behaviour is explained by assuming it is the result of a chain of independent efficient causes (components)

### **Control parameter** {- #Cont14}

+ A variable that controls the global behaviour of a dynamic system. For certain values of the parameter, transitions between qualitatively different behavioural modes (orders) can occur.

### **Critical fluctuations** {- #Crit15}

+ An early warning signal for a phase transition that is characterised by an increase in fluctuations (variability) of the behaviour of the system. The increase occurs because the self-organised transition from one state to another relaxes the constraints on the degrees of freedom a system has available to generate its behaviour, allowing states and behavioural modes to appear that were previously inaccessible.

### **Critical slowing down** {- #Crit16}

+ An early warning signal for a phase transition that is characterised by an increase in the duration of relaxation times. If it takes longer for the system to return to the state it was perturbed from, this implies the emergence of a new stable state is imminent

### **Deterministic Chaos** {- #Dete17}

+ Behaviour of a dynamic system that "looks random, but is not" (Lorenz, 1973). The dynamics can be characterised as follows: 1) A-periodic, no point or trajectory in state space will exactly recur; 2) Sensitive dependence in initial conditions; 3) Bounded, not all theoretically possible degrees of freedom are available to the system; 4) The origin of this behaviour is deterministic, not stochastic

### **Difference equation** {- #Diff18}

+ A function specifying the underlying change process in a variable from one discrete point in time to another

### **Differential equation** {- #Diff19}

+ A function specifying the underlying change process of a variable in continuous  time

### **Dimension** {- #Dime20}

+ See embedding dimension, box-counting dimension, correlation dimension, information dimension, dimension of a system

### **Dimensions of a system** {- #Dime21}

+ The set of variables that define a system. Iterative processes operate on the dimensions of a system

### **Dynamic system** {- #Dyna22}

+ A set of equations specifying how certain variables change over time. The equations specify how to determine (compute) the new values as a function of their current values and control parameters. The functions, when explicit, are either difference equations or differential equations. Dynamic systems may be stochastic or deterministic. In a stochastic system, new values come from a probability distribution. In a deterministic system, a single new value is associated with any current value

### **Early warning signals** {- #Earl23}

+ Critical slowing down and critical fluctuations. Early-warning signals indicate instability in the existing state which may result in a qualitative shift towards a new state (phase transition / catastrophe). Early-warning signals are similar to diagnostic catastrophe flags. 

### **Effective Complexity** {- #Effe24}

+ “The effective complexity of an entity is the length of a highly compressed description of its regularities.” (Gell-man & Lloyd, 2004)

### **Embedding Dimension** {- #Embe25}

+ Successive N-tuples of points in a time series are treated as points in N dimensional space. The points are said to reside in embedding dimensions of size N, for N = 1, 2, 3, 4, ... etc.

### **Emergence** {- #Emer26}

+ A complex system can generate emergent behaviour or display emergent properties that are novel and unexpected, that is, they are not predictable from the behaviour and properties of the components of the system

### **Entropy** {- #Entr27}

+ Relative absence of order/redundancy in a system. The degrees of freedom a system has available for generating its behaviour: Possibility

### **Epigenetic landscape (potential landscape)** {- #Epig28}

+ A hypothetical landscape describing the relative stability of behavioural modes of a system over time

### **Experienced event** {- #Expe29}

+ An interaction of a system with its environment that changed the internal structure/organization of the system such that it can be said to display adaptive behaviour. “Interaction with after-effects”. Random behaviour is “Interaction without after-effects”.

### **flow ~** {- #flow30}

+ A differential equation

### **Fractal** {- #Frac31}

+ An irregular shape with self-similarity. It has infinite detail, and cannot be differentiated. "Wherever chaos, turbulence, and disorder are found, fractal geometry is at play" (Briggs and Peat, 1989).

### **Fractal Dimension** {- #Frac32}

+ A measure of a geometric object that can take on fractional values. At first used as a synonym to Hausdorff dimension, fractal dimension is currently used as a more general term for a measure of how fast length, area, or volume increases with decrease in scale. (Peitgen, Jurgens, & Saupe, 1992a).

### **Graph theory** {- #Grap33}

+ Models in which associations between mathematical objects are defined as edges (connections) between vertices (nodes)

### **Hausdorff Dimension** {- #Haus34}

+ A measure of a geometric object that can take on fractional values. (see fractal dimension).

### **Holism (epistemic)** {- #Holi35}

+ "some property of a whole would be holistic if, according to the theory in question, there is no way we can find out about it using only local means, i.e., by using only all possible non-holistic resources available to an agent.” (Seevinck, 2002)

### **Idiographic approach** {- #Idio36}

+ Scientific explanation in which the goal is to generate knowledge about specific facts, events or entities. The goal is not to generalize to universal laws and first principles.

### **Information (quantity)** {- #Info37}

+ A measurable quantity that resolves uncertainty about the state of a system by assigning a value to the uncertainty.

### **Initial condition** {- #Init38}

+ The starting point of a dynamic system, the initial state of a system from which it evolved to the current state.

### **Interaction dominant dynamics** {- #Inte39}

+ A causal ontology in which observed behaviour is explained by assuming it is the result of interactions between processes across many temporal and spatial scales

### **Iteration** {- #Iter40}

+ The repeated application of a function, using its output from one application as its input for the next.

### **Iterative function** {- #Iter41}

+ A function used to calculate the new state of a dynamic system.

### **Iterative system** {- #Iter42}

+ A system in which one or more functions are iterated to define the system.

### **Largest Lyapunov exponent** {- #Larg43}

+ The value of the largest exponent in a spectrum of exponents (the Lyapunov spectrum), coefficients of time, that reflect the rate of departure (divergence) of dynamic orbits of a system. The largest exponent indicates the extent to which the behaviour of a system is sensitive to initial conditions.

### **Limit cycle** {- #Limi44}

+ An attractor that is periodic in time, that is, that cycles periodically through an ordered sequence of states.

### **Limit points** {- #Limi45}

+ Points in phase space. There are three kinds: attractors, repellors, and saddle points. A system moves away from repellors and towards attractors. A saddle point is both an attractor and a repellor, it attracts a system in certain regions, and repels the system to other regions.

### **Linear function of predictors** {- #Line46}

+ A linear equation is of predictors is of the form y=a*x(i)+b, in which variable y varies 'linearly' with other variables x(i). In this equation, 'a' determines the slope of the line and 'b' reflects the y-intercept, the value y obtains when all x(i) equal zero.

### **Linear function of time** {- #Line47}

+ A linear function of time is of the form ŷ(t) = a*y(t) + b, in which variable y varies 'linearly' with time 't', that is, with itself at an earlier moment in time. In this equation 'a' determines the rate with which 'y' will change as time passes, 'b' reflects the initial condition, the value y obtains when t equals zero.

### **map ...** {- #map48}

+ A difference equation

### **Nonlinear dynamics** {- #Nonl49}

+ The study of dynamic systems whose functions specify that change is not a linear function of time.

### **Orbit (trajectory)** {- #Orbi50}

+ A sequence of coordinates (a path) through the phase space of a system.

### **Order** {- #Orde51}

+ “order is essentially the arrival of redundancy in a system, a reduction of possibilities”(Von Förster, 2003). Any form of non-random association or dependency that exists between parts of a system, its behaviour over time and/or its environment is a form of order. In scientific explanation of behaviour, the presence of order in non-artificial systems must be explained and should not be (implicitly) assumed.

### **Order Parameter** {- #Orde52}

+ A nominal variable that indexes qualitatively different behavioural modes of a system, for example the phases of matter (gas, liquid, solid, plasma)

### **Period-doubling** {- #Peri53}

+ The change in dynamics in which a N-point attractor is replaced by a 2N-point attractor.

### **Phase portrait** {- #Phas54}

+ The collection of all trajectories from all possible starting points in the phase space of a dynamic system.

### **Phase space** {- #Phas55}

+ An abstract space used to represent the behaviour of a system. Its dimensions are the variables of the system. Thus a point in the phase space defines a potential state of the system. The points actually achieved by a system depend on its iterative function and initial condition (starting point).

### **Phase transition** {- #Phas56}

+ A transition between qualitatively different behavioural modes

### **Potential function** {- #Pote57}

+ A function that describes the order parameter of a system, that is, it describes the relative stability of the potential end-states (attractor states) a system can settle into. The parameters of the potential function include the control parameter.

### **Power-law scaling** {- #Powe58}

+ A relationship between two variables that is linear on doubly logarithmic coordinates, meaning the law is expressed in increments that represent 'power'

### **Recursive process** {- #Recu59}

+ For our purposes, "recursive" and "iterative" are synonyms. Thus recursive processes are iterative processes, and recursive functions are iterative functions.

### **Relaxation time** {- #Rela60}

+ The time it takes for a system to return to a stable state after it was perturbed enough to leave that state. A characteristic warning signal of an imminent phase transition is an increase relaxation times, also known as critical slowing down.

### **Repellors** {- #Repe61}

+ One type of limit point. A point in phase space that a system moves away from.

### **Return map** {- #Retu62}

+ Plot of time series values vs. a delayed copy of itself. A return plot can be used to get an idea about the functional form of the iterative process, it is a simple variant of delay embedding.

### **Saddle point** {- #Sadd63}

+ A point, usually in three dimensional state space, that both attracts and repels, attracting in one dimension and repelling to another.

### **Scale free network** {- #Scal64}

+ A network in which the distribution of the number of connections of a node and their frequency of occurrence follows a power-law in which there are just a few nodes with many connections and many nodes with just a few connections

### **Self-affinity** {- #Self65}

+ An infinite nesting of characteristic structure on all scales. Strict self-affinity refers to a form of which all substructures are affine transformation, which means the different dimensions of the system can be scaled by their own exponent. Statistical self-affinity refers to an approximate equivalence of form at all scales.

### **Self-similarity** {- #Self66}

+ An infinite nesting of characteristic structure on all scales. Strict self-similarity refers to a form of which all substructures can be considered scaling transformations, larger or smaller copies scaled by a single exponent for all dimensions of the structure. Statistical self-similarity refers to an approximate equivalence of scaled structure.

### **Sensitive dependence on initial conditions** {- #Sens67}

+ A property of chaotic systems. A dynamic system has sensitivity to initial conditions when very small differences in starting values result in very different behaviour. If the orbits of nearby starting points diverge, the system has sensitivity to initial conditions.

### **Small world network** {- #Smal68}

+ Many real-world networks have a small average shortest path length, but also a clustering coefficient that is significantly higher than expected by chance. These networks are extremely efficient, each node in a very large network can still be reach in just a few steps (the 'six degrees of separation` phenomenon).

### **State** {- #Stat69}

+ A coordinate in state space designating the current status of a dynamic system. The elements of the coordinates are values on the dimensions of the system that span the state space.

### **State space** {- #Stat70}

+ A hypothetical space spanned by the dimensions of the system. Each combination of values of variables that represent the dimension is a state of the system, it is a coordinate in state space.

### **State space (phase space)** {- #Stat71}

+ An abstract space used to represent the behaviour of a system. Its dimensions are the variables of the system. Thus a point in the phase space defines a potential state of the system.

### **Strange attractor** {- #Stra72}

+ An attractor state representing chaotic dynamics: a-periodic, bounded, and sensitive dependence on initial conditions

### **System** {- #Syst73}

+ An entity that can be described as a composition of components according to some organising principle. Organising principles describe how parts of the system relate to the whole and.

### **Time series** {- #Time74}

+ A record of observations (data points) of behaviour over time.

### **Trajectory (orbit)** {- #Traj75}

+ A sequence of positions (path) of a system in its phase space. The path from its starting point (initial condition) to and within its attractor.

### **Transient time (transient behaviour)** {- #Tran76}

+ The time it takes for a system to transition from one stable state (behavioural mode, attractor state) into another, during which the system displays transient behaviour

